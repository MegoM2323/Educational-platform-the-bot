# Prometheus alert rules for OpenTelemetry and Jaeger monitoring

groups:
  # OpenTelemetry Collector Alerts
  - name: otel_collector_alerts
    interval: 30s
    rules:
      # Alert if collector is down
      - alert: OTELCollectorDown
        expr: up{job="otel-collector"} == 0
        for: 2m
        annotations:
          summary: "OpenTelemetry Collector is down"
          description: "OpenTelemetry Collector has been down for more than 2 minutes"

      # Alert if high span drop rate
      - alert: OTELHighSpanDropRate
        expr: |
          (
            rate(otelcol_receiver_accepted_spans_total[5m]) -
            rate(otelcol_exporter_sent_spans_total[5m])
          ) / rate(otelcol_receiver_accepted_spans_total[5m]) > 0.1
        for: 5m
        annotations:
          summary: "OpenTelemetry Collector dropping spans"
          description: "Span drop rate is above 10%"

      # Alert if exporter queue is full
      - alert: OTELExporterQueueFull
        expr: otelcol_exporter_queue_size > 1800
        for: 5m
        annotations:
          summary: "OpenTelemetry Exporter queue is full"
          description: "Exporter queue is at {{ $value }} / 2000 capacity"

      # Alert if collector memory usage is high
      - alert: OTELCollectorHighMemory
        expr: |
          process_resident_memory_bytes{job="otel-collector"} > 512 * 1024 * 1024
        for: 5m
        annotations:
          summary: "OpenTelemetry Collector high memory usage"
          description: "Memory usage is {{ $value | humanize }}B"

  # Jaeger Alerts
  - name: jaeger_alerts
    interval: 30s
    rules:
      # Alert if Jaeger is down
      - alert: JaegerDown
        expr: up{job="jaeger"} == 0
        for: 2m
        annotations:
          summary: "Jaeger is down"
          description: "Jaeger has been down for more than 2 minutes"

      # Alert if Elasticsearch connection fails
      - alert: JaegerElasticsearchConnectionFailed
        expr: |
          increase(jaeger_elasticsearch_client_errors_total[5m]) > 10
        for: 5m
        annotations:
          summary: "Jaeger Elasticsearch connection errors"
          description: "Elasticsearch errors detected: {{ $value }} in 5m"

      # Alert if queue depth is high
      - alert: JaegerHighQueueDepth
        expr: |
          jaeger_collector_queue_length > 100
        for: 5m
        annotations:
          summary: "Jaeger collector queue depth high"
          description: "Queue depth is {{ $value }} spans"

  # Tracing Performance Alerts
  - name: tracing_performance
    interval: 30s
    rules:
      # Alert if trace latency is high
      - alert: HighTraceLatency
        expr: |
          histogram_quantile(0.95, rate(otelcol_exporter_sent_spans_total[5m])) > 0.1
        for: 5m
        annotations:
          summary: "High trace export latency"
          description: "P95 latency is above 100ms"

      # Alert if trace loss detected
      - alert: TraceDataLoss
        expr: |
          increase(otelcol_receiver_refused_spans_total[5m]) > 0
        for: 1m
        annotations:
          summary: "Trace data being dropped"
          description: "{{ $value }} spans refused in 5m"

# Recording rules for common metrics
  - name: tracing_recording_rules
    interval: 30s
    rules:
      # Span processing rate
      - record: tracing:spans_per_second
        expr: |
          rate(otelcol_receiver_accepted_spans_total[1m])

      # Trace export success rate
      - record: tracing:export_success_rate
        expr: |
          rate(otelcol_exporter_sent_spans_total[5m]) /
          rate(otelcol_receiver_accepted_spans_total[5m])

      # Collector resource utilization
      - record: tracing:collector_cpu_usage
        expr: |
          rate(process_cpu_seconds_total{job="otel-collector"}[1m])

      # Jaeger storage latency
      - record: tracing:jaeger_storage_latency_ms
        expr: |
          jaeger_storage_latency_ms
