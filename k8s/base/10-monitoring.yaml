---
# ServiceMonitor for Prometheus (requires Prometheus Operator)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: backend
  namespace: thebot
  labels:
    app: backend
spec:
  selector:
    matchLabels:
      app: backend
  endpoints:
  - port: http
    path: /api/system/metrics/prometheus/
    interval: 30s
    scrapeTimeout: 10s

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: postgres
  namespace: thebot
  labels:
    app: postgres
spec:
  selector:
    matchLabels:
      app: postgres
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: redis
  namespace: thebot
  labels:
    app: redis
spec:
  selector:
    matchLabels:
      app: redis
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s

---
# PrometheusRule for alerting
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: thebot-alerts
  namespace: thebot
  labels:
    app: thebot
    prometheus: default
spec:
  groups:
  - name: thebot.rules
    interval: 30s
    rules:
    # Backend alerts
    - alert: BackendPodRestartingTooOften
      expr: rate(kube_pod_container_status_restarts_total{pod=~"backend-.*"}[15m]) > 0.1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Backend pod restarting too often"
        description: "Backend pod {{ $labels.pod }} is restarting more than expected"

    - alert: BackendHighErrorRate
      expr: rate(django_http_requests_total{status=~"5.."}[5m]) > 0.05
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "High error rate in backend"
        description: "Error rate is {{ $value }} errors/sec"

    - alert: BackendHighResponseTime
      expr: django_http_requests_latency_seconds_bucket{le="1.0"} < 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High response time in backend"
        description: "Response time is higher than expected"

    # Database alerts
    - alert: PostgreSQLDown
      expr: up{job="postgres"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "PostgreSQL is down"
        description: "PostgreSQL has been down for more than 1 minute"

    - alert: PostgreSQLTooManyConnections
      expr: |
        sum by (instance) (pg_stat_activity_count{state="active"})
        /
        pg_setting_max_connections * 100 > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PostgreSQL has too many connections"
        description: "PostgreSQL is using {{ $value | humanize }}% of available connections"

    - alert: PostgreSQLHighDiskUsage
      expr: |
        (pg_database_size_bytes / node_filesystem_avail_bytes) * 100 > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PostgreSQL disk usage is high"
        description: "Disk usage is {{ $value | humanize }}%"

    - alert: PostgreSQLSlowQueries
      expr: |
        rate(pg_slow_queries_total[5m]) > 0.5
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PostgreSQL has slow queries"
        description: "Slow query rate is {{ $value | humanize }} queries/sec"

    # Redis alerts
    - alert: RedisDown
      expr: up{job="redis"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Redis is down"
        description: "Redis has been down for more than 1 minute"

    - alert: RedisHighMemoryUsage
      expr: |
        redis_memory_used_bytes /
        redis_memory_max_bytes * 100 > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Redis memory usage is high"
        description: "Memory usage is {{ $value | humanize }}%"

    - alert: RedisHighKeyEvictions
      expr: |
        rate(redis_evicted_keys_total[5m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Redis is evicting keys"
        description: "Eviction rate is {{ $value | humanize }} keys/sec"

    # Kubernetes alerts
    - alert: PodCrashLooping
      expr: |
        rate(kube_pod_container_status_restarts_total{namespace="thebot"}[5m]) > 0.1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Pod is crash looping"
        description: "Pod {{ $labels.pod }} is restarting frequently"

    - alert: PodNotHealthy
      expr: |
        min_over_time(sum by (namespace, pod) (kube_pod_status_phase{namespace="thebot",phase=~"Pending|Unknown|Failed"})[15m:1m]) > 0
      for: 15m
      labels:
        severity: critical
      annotations:
        summary: "Pod is not healthy"
        description: "Pod {{ $labels.pod }} has been in a non-healthy state for more than 15 minutes"

    - alert: DeploymentReplicasMismatch
      expr: |
        kube_deployment_spec_replicas{namespace="thebot"} !=
        kube_deployment_status_replicas_available{namespace="thebot"}
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Deployment replicas mismatch"
        description: "Deployment {{ $labels.deployment }} has mismatched replicas"

    - alert: PVCPending
      expr: |
        kube_persistentvolumeclaim_status_phase{namespace="thebot",phase="Pending"} == 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PVC is pending"
        description: "PVC {{ $labels.persistentvolumeclaim }} is pending"

    - alert: PVCAlmostFull
      expr: |
        (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PVC is almost full"
        description: "PVC {{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full"

    # Ingress alerts
    - alert: IngressHighErrorRate
      expr: |
        sum(rate(nginx_ingress_controller_requests{namespace="thebot",status=~"5.."}[5m]))
        /
        sum(rate(nginx_ingress_controller_requests{namespace="thebot"}[5m])) > 0.05
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Ingress has high error rate"
        description: "Error rate is {{ $value | humanizePercentage }}"

---
# PrometheusRule for recording rules
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: thebot-recording-rules
  namespace: thebot
  labels:
    app: thebot
    prometheus: default
spec:
  groups:
  - name: thebot.recording
    interval: 30s
    rules:
    - record: job:django_http_requests_total:rate5m
      expr: rate(django_http_requests_total[5m])

    - record: job:django_http_requests_latency_seconds:p95
      expr: |
        histogram_quantile(0.95, django_http_requests_latency_seconds_bucket)

    - record: job:postgres_connections:ratio
      expr: |
        sum(pg_stat_activity_count) / pg_setting_max_connections

    - record: job:redis_memory:ratio
      expr: |
        redis_memory_used_bytes / redis_memory_max_bytes

---
# AlertmanagerConfig for routing alerts
apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
metadata:
  name: thebot-alerts
  namespace: thebot
spec:
  route:
    receiver: 'thebot-alerts'
    groupBy: ['alertname', 'job']
    groupWait: 30s
    groupInterval: 5m
    repeatInterval: 12h
    routes:
    - matchers:
      - name: severity
        value: critical
      receiver: 'critical-alerts'
      repeatInterval: 5m
    - matchers:
      - name: severity
        value: warning
      receiver: 'warning-alerts'
      repeatInterval: 1h

  receivers:
  - name: 'thebot-alerts'
    webhookConfigs:
    - url: 'http://localhost:5001/'

  - name: 'critical-alerts'
    emailConfigs:
    - to: admin@the-bot.ru
      from: alerts@the-bot.ru
      smarthost: 'smtp.gmail.com:587'
      authUsername: 'alerts@the-bot.ru'
      authPassword: 'password'
      headers:
        Subject: 'Critical Alert: {{ .GroupLabels.alertname }}'

  - name: 'warning-alerts'
    emailConfigs:
    - to: team@the-bot.ru
      from: alerts@the-bot.ru
      smarthost: 'smtp.gmail.com:587'
      authUsername: 'alerts@the-bot.ru'
      authPassword: 'password'
      headers:
        Subject: 'Warning Alert: {{ .GroupLabels.alertname }}'
