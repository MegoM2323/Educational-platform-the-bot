# Logstash configuration for THE_BOT platform
# Processes logs from Django, Nginx, Celery, PostgreSQL, Redis, and system

input {
  # Read from log files
  file {
    id => "django-app-logs"
    path => "/var/log/thebot/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => multiline {
      pattern => "^\["
      negate => true
      what => "previous"
    }
    tags => ["django", "app"]
  }

  # TCP input for direct application logging
  tcp {
    port => 5000
    codec => json
    tags => ["tcp-input"]
  }

  # UDP input for syslog
  udp {
    port => 5000
    codec => plain
    tags => ["syslog"]
  }

  # Beats input for Filebeat
  beats {
    port => 5044
    codec => plain
  }
}

filter {
  # Parse Django logs
  if "django" in [tags] {
    # Parse timestamp
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\]" }
      tag_on_failure => ["grok_timestamp_failed"]
    }

    # Parse log level
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601}\]\s+\[%{LOGLEVEL:log_level}\]" }
      tag_on_failure => ["grok_loglevel_failed"]
    }

    # Parse Django logger name
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601}\]\s+\[%{LOGLEVEL}\]\s+%{DATA:logger_name}" }
      tag_on_failure => ["grok_logger_failed"]
    }

    # Parse HTTP requests
    if "request" in [message] or "GET" in [message] or "POST" in [message] or "PUT" in [message] or "DELETE" in [message] {
      grok {
        match => { "message" => "%{WORD:http_method}\s+%{DATA:http_path}\s+%{WORD:http_protocol}/%{NUMBER:http_version}" }
        tag_on_failure => ["grok_http_failed"]
      }

      grok {
        match => { "message" => "Status:\s+%{NUMBER:http_status:int}" }
        tag_on_failure => ["grok_status_failed"]
      }

      grok {
        match => { "message" => "Time:\s+%{NUMBER:response_time:float}ms" }
        tag_on_failure => ["grok_time_failed"]
      }
    }

    # Parse authentication/security events
    if "auth" in [message] or "login" in [message] or "token" in [message] {
      grok {
        match => { "message" => "user[_id]*[=:]\s*%{DATA:user_id}" }
        tag_on_failure => ["grok_user_failed"]
      }

      mutate {
        add_tag => ["security", "auth"]
      }
    }

    # Parse error traces
    if [log_level] == "ERROR" or [log_level] == "CRITICAL" {
      mutate {
        add_tag => ["error", "alert"]
      }

      # Extract exception type
      grok {
        match => { "message" => "(?<exception_type>%{DATA}):\s+%{GREEDYDATA:exception_message}" }
        tag_on_failure => ["grok_exception_failed"]
      }
    }

    # Parse database queries
    if "query" in [message] or "connection" in [message] {
      mutate {
        add_tag => ["database"]
      }
    }

    # Convert timestamp
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
      tag_on_failure => ["date_parse_failed"]
    }
  }

  # Parse Celery logs
  if "celery" in [message] or "celery" in [tags] {
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\s+%{INT}\]\s+%{LOGLEVEL:log_level}\s+%{DATA:task_name}" }
      tag_on_failure => ["grok_celery_failed"]
    }

    # Extract task ID
    grok {
      match => { "message" => "task\s+id=<%{DATA:task_id}>" }
      tag_on_failure => ["grok_taskid_failed"]
    }

    # Extract execution time
    grok {
      match => { "message" => "(\d+\.\d+)s" }
      tag_on_failure => ["grok_exec_time_failed"]
    }

    mutate {
      add_tag => ["celery", "async-task"]
    }

    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
      tag_on_failure => ["date_parse_failed"]
    }
  }

  # Parse Nginx logs
  if "nginx" in [message] or "nginx" in [tags] {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
      tag_on_failure => ["grok_nginx_failed"]
    }

    mutate {
      convert => { "bytes" => "integer" }
      convert => { "response" => "integer" }
      rename => { "response" => "http_status" }
    }

    mutate {
      add_tag => ["nginx", "web-server"]
    }
  }

  # Parse PostgreSQL logs
  if "postgres" in [message] or "postgres" in [tags] {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+\[%{INT:postgres_pid}\]\s+%{WORD:postgres_user}@%{DATA:postgres_db}\s+%{LOGLEVEL:log_level}:\s+%{GREEDYDATA:postgres_message}" }
      tag_on_failure => ["grok_postgres_failed"]
    }

    mutate {
      add_tag => ["postgres", "database"]
    }

    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
      tag_on_failure => ["date_parse_failed"]
    }
  }

  # Parse Redis logs
  if "redis" in [message] or "redis" in [tags] {
    grok {
      match => { "message" => "%{INT:redis_port}:M\s+%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:log_level}\s+%{GREEDYDATA:redis_message}" }
      tag_on_failure => ["grok_redis_failed"]
    }

    mutate {
      add_tag => ["redis", "cache"]
    }

    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
      tag_on_failure => ["date_parse_failed"]
    }
  }

  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "json_payload"
      tag_on_failure => ["json_parse_failed"]
    }

    # Extract fields from JSON payload
    if [json_payload] {
      mutate {
        add_field => { "service" => "%{[json_payload][service]}" }
        add_field => { "request_id" => "%{[json_payload][request_id]}" }
        add_field => { "user_id" => "%{[json_payload][user_id]}" }
        add_field => { "duration_ms" => "%{[json_payload][duration_ms]}" }
      }
    }
  }

  # Parse system logs
  if "syslog" in [tags] {
    grok {
      match => { "message" => "%{SYSLOGLINE}" }
      tag_on_failure => ["grok_syslog_failed"]
    }

    mutate {
      add_tag => ["system", "os"]
    }
  }

  # Common field processing
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
      tag_on_failure => ["date_parse_failed"]
    }
  }

  # Add metadata
  mutate {
    add_field => { "environment" => "development" }
    add_field => { "platform" => "thebot" }
    add_field => { "version" => "1.0.0" }
  }

  # Extract service from tags or message
  if ![service] {
    if "django" in [tags] {
      mutate { add_field => { "service" => "django-backend" } }
    } else if "celery" in [tags] {
      mutate { add_field => { "service" => "celery-worker" } }
    } else if "nginx" in [tags] {
      mutate { add_field => { "service" => "nginx" } }
    } else if "postgres" in [tags] {
      mutate { add_field => { "service" => "postgresql" } }
    } else if "redis" in [tags] {
      mutate { add_field => { "service" => "redis" } }
    } else if "system" in [tags] {
      mutate { add_field => { "service" => "system" } }
    }
  }

  # Add hostname
  if ![hostname] {
    mutate {
      add_field => { "hostname" => "%{host}" }
    }
  }

  # Clean up temporary fields
  mutate {
    remove_field => [ "beat" ]
  }
}

output {
  # Send to Elasticsearch with time-based indices
  elasticsearch {
    id => "elasticsearch-output"
    hosts => ["${ELASTICSEARCH_HOSTS:http://elasticsearch:9200}"]
    index => "thebot-logs-%{+YYYY.MM.dd}"
    doc_type => "_doc"
    template => "/usr/share/logstash/templates/thebot-template.json"
    template_name => "thebot-logs"
    template_overwrite => true
    retry_on_conflict => 3
    action => "index"
  }

  # Debug output to console (optional)
  if [@metadata][debug] {
    stdout {
      codec => rubydebug
    }
  }

  # Send errors to separate index for alerting
  if "error" in [tags] or "alert" in [tags] or [log_level] == "ERROR" or [log_level] == "CRITICAL" {
    elasticsearch {
      id => "elasticsearch-error-output"
      hosts => ["${ELASTICSEARCH_HOSTS:http://elasticsearch:9200}"]
      index => "thebot-errors-%{+YYYY.MM.dd}"
      doc_type => "_doc"
      retry_on_conflict => 3
      action => "index"
    }
  }
}
