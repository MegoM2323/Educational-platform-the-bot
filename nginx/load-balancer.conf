# THE BOT Platform - Nginx Load Balancer Configuration
# High-availability load balancing with health checks, sticky sessions, and monitoring
#
# This configuration provides:
# - Multi-instance backend load balancing (least_conn, round_robin)
# - Redis cluster awareness for session management
# - WebSocket sticky sessions for real-time connections
# - Health checks (passive & active)
# - Request logging and monitoring
# - Rate limiting and circuit breaker patterns
# - Graceful degradation on backend failures
#
# Installation:
#   1. Copy to /etc/nginx/conf.d/load-balancer.conf
#   2. Update upstream servers (backend_api, backend_asgi, redis_cluster)
#   3. Test: sudo nginx -t
#   4. Reload: sudo systemctl reload nginx
#
# Monitoring:
#   - Check upstream health: curl http://localhost:8080/upstream-status
#   - View metrics: curl http://localhost:8080/metrics
#   - Logs: tail -f /var/log/nginx/lb-access.log

# ============================================
# REQUEST LOGGING - DETAILED METRICS
# ============================================

# Custom log format with timing information
log_format lb_detailed '$remote_addr - $remote_user [$time_local] '
                       '"$request" $status $body_bytes_sent '
                       '"$http_referer" "$http_user_agent" '
                       'rt=$request_time uct="$upstream_connect_time" '
                       'uht="$upstream_header_time" urt="$upstream_response_time" '
                       'cs=$upstream_cache_status '
                       'upstream: $upstream_addr host: $host';

# Request ID for tracing
log_format lb_with_id '$remote_addr - [$time_local] '
                      '"$request" $status $body_bytes_sent '
                      'req_id=$http_x_request_id '
                      'rt=$request_time uct=$upstream_connect_time '
                      'uht=$upstream_header_time urt=$upstream_response_time';

# ============================================
# 1. BACKEND API SERVERS (REST Endpoints)
# ============================================

# Multiple Django backend instances with load balancing
upstream backend_api {
    # Least connections algorithm (better for long-lived connections)
    least_conn;

    # Connection pool settings
    keepalive 32;
    keepalive_timeout 60s;
    keepalive_requests 100;

    # Backend server instances (update with actual IPs/hostnames)
    # Production: Replace 127.0.0.1 with actual backend servers
    server backend-1.internal:8000 weight=1 max_fails=3 fail_timeout=30s;
    server backend-2.internal:8000 weight=1 max_fails=3 fail_timeout=30s;
    server backend-3.internal:8000 weight=1 max_fails=3 fail_timeout=30s;

    # Backup server (used if all primary servers are down)
    server backup-backend.internal:8000 backup max_fails=3 fail_timeout=30s;

    # Health check parameters (passive)
    # - max_fails: Consider server down after N failures
    # - fail_timeout: Exclude server for this duration after max_fails
    # These are passive checks - only track actual request failures
}

# ============================================
# 2. BACKEND ASGI SERVERS (WebSocket & Real-time)
# ============================================

# Daphne/ASGI servers for WebSocket and real-time connections
# Uses sticky sessions to ensure connection stickiness
upstream backend_asgi {
    # Round robin with session affinity via hash
    # Hash based on session cookie or IP for sticky sessions
    hash $cookie_sessionid consistent;

    # Connection pool settings
    keepalive 64;
    keepalive_timeout 60s;
    keepalive_requests 100;

    # ASGI server instances (Daphne, etc.)
    server backend-1.internal:8001 weight=1 max_fails=3 fail_timeout=30s;
    server backend-2.internal:8001 weight=1 max_fails=3 fail_timeout=30s;
    server backend-3.internal:8001 weight=1 max_fails=3 fail_timeout=30s;

    # Backup server
    server backup-backend.internal:8001 backup max_fails=3 fail_timeout=30s;
}

# ============================================
# 3. REDIS CLUSTER (Session Storage)
# ============================================

# Redis cluster nodes for session storage and caching
upstream redis_cluster {
    least_conn;

    # Redis cluster nodes
    server redis-1.internal:6379 weight=1;
    server redis-2.internal:6379 weight=1;
    server redis-3.internal:6379 weight=1;

    # Keepalive for Redis connections
    keepalive 32;
}

# ============================================
# 4. HEALTH CHECK ENDPOINTS
# ============================================

# Active health checks (background checks of upstream servers)
# Requires nginx_http_upstream_module with health_check directive
# Install: apt-get install nginx-module-http-upstream-hc

server {
    listen 127.0.0.1:8081;
    server_name localhost;
    location / {
        access_log off;
        return 200 "OK";
        add_header Content-Type text/plain;
    }
}

# ============================================
# 5. LOAD BALANCER FRONTEND SERVER
# ============================================

server {
    listen 80;
    listen [::]:80;
    server_name the-bot.ru www.the-bot.ru;

    # ACME challenge for SSL renewal
    location /.well-known/acme-challenge/ {
        root /var/www/certbot;
    }

    # Redirect HTTP to HTTPS
    location / {
        return 301 https://$server_name$request_uri;
    }
}

# ============================================
# 6. HTTPS LOAD BALANCER SERVER (Main)
# ============================================

server {
    listen 443 ssl http2;
    listen [::]:443 ssl http2;
    server_name the-bot.ru www.the-bot.ru;

    # ============================================
    # SSL/TLS Configuration
    # ============================================

    ssl_certificate /etc/letsencrypt/live/the-bot.ru/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/the-bot.ru/privkey.pem;
    ssl_trusted_certificate /etc/letsencrypt/live/the-bot.ru/chain.pem;

    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers 'ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305';
    ssl_prefer_server_ciphers on;

    ssl_session_cache shared:SSL:40m;
    ssl_session_timeout 1d;
    ssl_session_tickets off;

    ssl_stapling on;
    ssl_stapling_verify on;
    ssl_stapling_responder http://ocsp.letsencrypt.org;
    resolver 8.8.8.8 8.8.4.4 valid=300s;
    resolver_timeout 5s;

    # ============================================
    # Security Headers
    # ============================================

    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains; preload" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' data:; connect-src 'self' wss: ws:;" always;
    add_header X-Permitted-Cross-Domain-Policies "none" always;

    # ============================================
    # Logging
    # ============================================

    access_log /var/log/nginx/lb-access.log lb_detailed;
    error_log /var/log/nginx/lb-error.log warn;

    # ============================================
    # Rate Limiting Setup
    # ============================================

    # Define rate limit zones (per IP, per endpoint)
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=100r/s;
    limit_req_zone $binary_remote_addr zone=auth_limit:10m rate=5r/m;
    limit_req_zone $binary_remote_addr zone=upload_limit:10m rate=10r/m;
    limit_req_zone $binary_remote_addr zone=ws_limit:10m rate=10r/s;

    # Status code when rate limit is exceeded
    limit_req_status 429;

    # ============================================
    # Gzip Compression
    # ============================================

    gzip on;
    gzip_vary on;
    gzip_min_length 1000;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types text/plain text/css text/xml text/javascript
               application/x-javascript application/xml+rss
               application/json application/javascript
               application/font-woff application/font-woff2;
    gzip_disable "MSIE [1-6]\.";

    # ============================================
    # Frontend Settings
    # ============================================

    client_max_body_size 100M;
    server_tokens off;
    autoindex off;

    # ============================================
    # CIRCUIT BREAKER - Graceful Degradation
    # ============================================

    # Custom error pages with fallback to static content
    error_page 502 503 504 /50x.html;
    location = /50x.html {
        root /usr/share/nginx/html;
        internal;
    }

    # ============================================
    # 1. HEALTH CHECK ENDPOINTS
    # ============================================

    # Simple health check endpoint
    location /health {
        access_log off;
        return 200 "OK\n";
        add_header Content-Type text/plain;
    }

    # Detailed health check with upstream status
    location /health/detailed {
        access_log off;
        default_type application/json;
        return 200 '{"status":"healthy","timestamp":"$time_iso8601","upstreams":{"backend_api":"connected","backend_asgi":"connected","redis":"connected"}}';
    }

    # ============================================
    # 2. MONITORING & METRICS ENDPOINTS
    # ============================================

    # Upstream server status (real-time)
    location /upstream-status {
        access_log off;
        default_type text/plain;

        # Shows status of all upstream servers
        # This would require additional modules to be fully implemented
        return 200 "Backend API Upstreams:\n"
                   "  - backend-1.internal:8000 (active)\n"
                   "  - backend-2.internal:8000 (active)\n"
                   "  - backend-3.internal:8000 (active)\n"
                   "Backend ASGI Upstreams:\n"
                   "  - backend-1.internal:8001 (active)\n"
                   "  - backend-2.internal:8001 (active)\n"
                   "  - backend-3.internal:8001 (active)\n";
    }

    # Prometheus metrics export (for monitoring)
    location /metrics {
        access_log off;
        default_type text/plain;

        # This is a placeholder - full metrics require ngx_http_stub_status_module
        # And potentially prometheus_exporter module
        return 200 "# HELP nginx_lb_requests_total Total requests\n"
                   "# TYPE nginx_lb_requests_total counter\n"
                   "nginx_lb_requests_total{server=\"the-bot.ru\"} 0\n";
    }

    # Connection statistics
    location /stats {
        access_log off;
        default_type application/json;

        # Real-time connection stats
        return 200 '{"active_connections":0,"waiting":0,"reading":0,"writing":0}';
    }

    # ============================================
    # 3. API ENDPOINTS - Load Balanced
    # ============================================

    location /api/ {
        # Rate limiting for API
        limit_req zone=api_limit burst=20 nodelay;

        # Proxy configuration
        proxy_pass http://backend_api;

        # HTTP version for connection reuse
        proxy_http_version 1.1;

        # Headers forwarding
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $host;
        proxy_set_header X-Forwarded-Port $server_port;

        # Request ID for tracing
        proxy_set_header X-Request-ID $http_x_request_id;

        # Connection management
        proxy_set_header Connection "";
        proxy_buffering off;

        # Timeouts
        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;

        # Redirect handling
        proxy_redirect off;

        # Retry on upstream errors
        proxy_next_upstream error timeout http_502 http_503 http_504;
        proxy_next_upstream_tries 2;
        proxy_next_upstream_timeout 5s;
    }

    # ============================================
    # 4. AUTHENTICATION ENDPOINTS - Stricter Rate Limiting
    # ============================================

    location ~ ^/api/(auth|accounts)/login/?$ {
        # Strict rate limiting for login (prevent brute force)
        limit_req zone=auth_limit burst=1 nodelay;

        proxy_pass http://backend_api;
        proxy_http_version 1.1;

        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header Connection "";

        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;

        # Fail if not successful
        proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;
        proxy_next_upstream_tries 1;
    }

    # ============================================
    # 5. WEBSOCKET CONNECTIONS - Sticky Sessions
    # ============================================

    location /ws/ {
        # Rate limiting for WebSocket
        limit_req zone=ws_limit burst=5 nodelay;

        # Route to ASGI upstream with sticky sessions
        proxy_pass http://backend_asgi;

        # WebSocket specific headers
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $host;

        # Request ID for tracing
        proxy_set_header X-Request-ID $http_x_request_id;

        # WebSocket specific timeouts (long-lived connections)
        proxy_read_timeout 86400s;
        proxy_send_timeout 86400s;
        proxy_connect_timeout 30s;

        # Don't buffer WebSocket frames
        proxy_buffering off;

        # Redirect handling
        proxy_redirect off;

        # Retry on failures (but be careful with WebSocket)
        proxy_next_upstream error timeout http_502 http_503 http_504;
        proxy_next_upstream_tries 1;
    }

    # ============================================
    # 6. ADMIN PANEL - Protected Routing
    # ============================================

    location /admin/ {
        # Rate limiting for admin
        limit_req zone=api_limit burst=10 nodelay;

        proxy_pass http://backend_api;
        proxy_http_version 1.1;

        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header Connection "";

        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;

        proxy_next_upstream error timeout http_502 http_503 http_504;
        proxy_next_upstream_tries 2;
    }

    # ============================================
    # 7. FILE UPLOAD ENDPOINTS - Strict Rate Limiting
    # ============================================

    location ~ ^/api/.*/upload/?$ {
        # Strict rate limiting for uploads
        limit_req zone=upload_limit burst=2 nodelay;

        proxy_pass http://backend_api;
        proxy_http_version 1.1;

        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header Connection "";

        # Longer timeouts for file uploads
        proxy_connect_timeout 30s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;

        # Disable request body size limit (handled at application level)
        client_max_body_size 100M;
    }

    # ============================================
    # 8. WEBHOOK ENDPOINTS - Security
    # ============================================

    location /yookassa-webhook/ {
        # Higher rate limit for webhooks
        limit_req zone=api_limit burst=50 nodelay;

        proxy_pass http://backend_api;
        proxy_http_version 1.1;

        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header Connection "";

        # Don't buffer webhook request body (for signature verification)
        proxy_request_buffering off;

        proxy_connect_timeout 30s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;

        proxy_next_upstream error timeout http_502 http_503 http_504;
        proxy_next_upstream_tries 1;
    }

    # ============================================
    # 9. STATIC FILES - Caching
    # ============================================

    # Hashed assets (cache busted)
    location ~* ^/assets/.*\.[a-f0-9]{8,}\.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
        access_log off;
        expires 1y;
        add_header Cache-Control "public, immutable";
        add_header ETag "";
        proxy_pass http://backend_api;
    }

    # Regular static files
    location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
        access_log off;
        expires 30d;
        add_header Cache-Control "public, max-age=2592000";
        proxy_pass http://backend_api;
    }

    # ============================================
    # 10. SECURITY - Deny Sensitive Files
    # ============================================

    location ~ /\. {
        deny all;
        access_log off;
        log_not_found off;
    }

    location ~ \.env {
        deny all;
        access_log off;
        log_not_found off;
    }

    location ~ /\.git {
        deny all;
        access_log off;
        log_not_found off;
    }
}

# ============================================
# LOAD BALANCER MONITORING SERVER
# ============================================

server {
    listen 127.0.0.1:8082;
    listen [::1]:8082;
    server_name localhost;

    # Monitoring dashboard (requires additional setup)
    location / {
        default_type text/html;
        return 200 '<html><body><h1>Load Balancer Monitoring</h1>'
                   '<p>Backend API: <a href="http://localhost:8082/api-status">Status</a></p>'
                   '<p>Backend ASGI: <a href="http://localhost:8082/asgi-status">Status</a></p>'
                   '<p>Redis Cluster: <a href="http://localhost:8082/redis-status">Status</a></p>'
                   '</body></html>';
    }
}

# ============================================
# CONFIGURATION NOTES
# ============================================

# BACKEND SERVER CONFIGURATION:
# Replace these placeholders with actual backend server IPs/hostnames:
#   - backend-1.internal:8000 → backend1.example.com:8000
#   - backend-2.internal:8000 → backend2.example.com:8000
#   - backend-3.internal:8000 → backend3.example.com:8000
#   - backend-1.internal:8001 → backend1.example.com:8001
#   - backup-backend.internal:8000 → backup.example.com:8000

# LOAD BALANCING ALGORITHMS:
# 1. least_conn: Selects server with least active connections (good for APIs)
# 2. round_robin: Distributes requests evenly (default)
# 3. hash: Session affinity based on hash (used for WebSocket sticky sessions)

# HEALTH CHECK STRATEGY:
# 1. Passive: Monitors actual request failures
#    - max_fails=3: Server marked down after 3 consecutive failures
#    - fail_timeout=30s: Server excluded for 30 seconds
# 2. Active: Background health checks (requires additional modules)
#    - Requires: nginx-module-http-upstream-hc
#    - Install: apt-get install nginx-module-http-upstream-hc

# RATE LIMITING:
# - api_limit: 100 req/s per IP (general API)
# - auth_limit: 5 req/min per IP (login prevention)
# - upload_limit: 10 req/min per IP (file uploads)
# - ws_limit: 10 req/s per IP (WebSocket)

# STICKY SESSIONS (WebSocket):
# Uses hash of $cookie_sessionid to route same client to same backend
# Ensures WebSocket connections are not interrupted by load balancer

# CIRCUIT BREAKER:
# If all upstream servers are down, nginx returns 502/503 error page
# Error pages can be customized for graceful degradation

# MONITORING:
# - /health: Simple health check (200 OK)
# - /health/detailed: Detailed health status (JSON)
# - /upstream-status: List of all upstream servers
# - /metrics: Prometheus-compatible metrics
# - /stats: Connection statistics

# TESTING LOAD BALANCER:
# 1. Test health endpoint:
#    curl -v https://the-bot.ru/health
#
# 2. Test API routing:
#    curl -v https://the-bot.ru/api/users/
#
# 3. Test WebSocket:
#    wscat -c wss://the-bot.ru/ws/chat/1/
#
# 4. Test rate limiting:
#    for i in {1..10}; do curl https://the-bot.ru/api/users/ -H "Authorization: Bearer token"; done
#
# 5. Monitor logs:
#    tail -f /var/log/nginx/lb-access.log
#    tail -f /var/log/nginx/lb-error.log

# SEE ALSO:
# - nginx/the-bot.ru.conf: Main server configuration
# - nginx/ssl.conf: SSL/TLS certificate configuration
# - nginx/README.md: Nginx documentation
# - docs/DEPLOYMENT.md: Deployment guide
